{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Creation - Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hHyMAEg5LzL5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input, Embedding\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "\n",
        "drive_path = '/content/drive/MyDrive/01_Applied_AI_Course_New/CaseStudies/Text Creation/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TextGenerator Pipleline"
      ],
      "metadata": {
        "id": "RBrJEhSfYaWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator():\n",
        "\n",
        "  def __init__(self, sample_input_text, predict_next_words=50, with_beam=False):\n",
        "    self.sample_input_text = sample_input_text\n",
        "    self.predict_next_words = predict_next_words\n",
        "    self.with_beam = with_beam\n",
        "\n",
        "  def load_model_weights(self):\n",
        "    ## load the model using trained weights\n",
        "\n",
        "    maxlen = 80\n",
        "    dropout = 0.2\n",
        "    rnn_units = 512\n",
        "    embed_dim = 256\n",
        "    unicode_vocab_size = 20000\n",
        "\n",
        "    model = WordRNN(unicode_vocab_size, embed_dim, maxlen, dropout, rnn_units, kernel_reg=None)\n",
        "\n",
        "    sample_input_batch = np.random.randint(low=1, high=unicode_vocab_size, size=(32,maxlen))\n",
        "    model(sample_input_batch)\n",
        "\n",
        "    model.load_weights(drive_path+'scifi_lstm_model_with_full_data_unicode_20k_vocab_size.hdf5')\n",
        "    return model\n",
        "\n",
        "  # generate text for next set of words using sample input text \n",
        "  def generate_text(self):\n",
        "    start = time.time()\n",
        "    \n",
        "    # load id_lookup & word_lookup\n",
        "    word_lookup = pd.read_pickle(drive_path+'scifi_unicode_vocab_dict.pkl')\n",
        "    id_lookup = pd.read_pickle(drive_path+'scifi_unicode_id_lookup.pkl')\n",
        "\n",
        "    h_states, c_states = None, None\n",
        "    sample_input = self.text_standardization(self.sample_input_text)\n",
        "\n",
        "    next_word = [sample_input]\n",
        "    result = [sample_input]\n",
        "\n",
        "    # load model \n",
        "    model = self.load_model_weights()\n",
        "    \n",
        "    # lstm model text generation\n",
        "    for n in tqdm(range(self.predict_next_words)):\n",
        "      next_word, h_states, c_states = self.gnerate_next_word(model, next_word, result, word_lookup, id_lookup, h_states=h_states, c_states=c_states, beam=self.with_beam)\n",
        "      result.extend(next_word)\n",
        "\n",
        "    result = ' '.join([_ for _ in result if _!='UNK'])\n",
        "    end = time.time()\n",
        "    print(f'\\nRun time:, {end - start}\\n')\n",
        "    return result\n",
        "\n",
        "  ## https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
        "\n",
        "  def gnerate_next_word(self, model, inputs, result, word_lookup, id_lookup, h_states=None, c_states=None, beam=False):\n",
        "\n",
        "    sample_index = len(result)-1\n",
        "\n",
        "    # Convert strings to token IDs.\n",
        "    input_ids = self.get_input_ids(result, word_lookup)\n",
        "\n",
        "    if beam:\n",
        "      predicted_logits, h_states, c_states = self.predict_one_step(model, input_ids, h_states, c_states)\n",
        "\n",
        "      # pick the highest log likelihood ratio among top 3 proposals\n",
        "      predicted_ids = self.beam_search_decoder(predicted_logits[:,sample_index,:], 3)\n",
        "      predicted_ids = predicted_ids[-1][0][0]\n",
        "    \n",
        "    else:\n",
        "      predicted_logits, h_states, c_states = self.predict_one_step(model, input_ids, h_states, c_states)\n",
        "      \n",
        "      # Convert from token ids to characters\n",
        "      predicted_ids = self.greedy_seach(predicted_logits[0][sample_index], id_lookup)\n",
        "    \n",
        "    predicted_word = id_lookup[predicted_ids]\n",
        "\n",
        "    # Return the word and model state.\n",
        "    return [predicted_word], h_states, c_states\n",
        "\n",
        "  # generate token ids for sample input \n",
        "  def get_input_ids(self, sample_input, word_lookup):\n",
        "    input = np.zeros(80, dtype=np.int64)\n",
        "    for idx, w in enumerate(sample_input):\n",
        "      input[idx]=word_lookup.get(w,1)\n",
        "    return tf.convert_to_tensor(input)\n",
        "\n",
        "  ##https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
        "\n",
        "  # beam search\n",
        "  def beam_search_decoder(self, data, k):\n",
        "    sequences = [[list(), 0.0]]\n",
        "    # walk over each step in sequence\n",
        "    for row in data:\n",
        "      all_candidates = list()\n",
        "      # expand each current candidate\n",
        "      for i in range(len(sequences)):\n",
        "        seq, score = sequences[i]\n",
        "        for j in range(len(row)):\n",
        "          candidate = [seq + [j], score - row[j]]\n",
        "          all_candidates.append(candidate)\n",
        "      # order all candidates by score\n",
        "      ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "      # select k best\n",
        "      sequences = ordered[:k]\n",
        "    return sequences\n",
        "\n",
        "  # Get the top preicted index based on logits\n",
        "  def greedy_seach(self, predicted_logits, id_lookup):\n",
        "    logits, indices = tf.math.top_k(predicted_logits, k=10, sorted=True)\n",
        "    indices = np.asarray(indices).astype(\"int32\")\n",
        "    preds = tf.keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "    preds = np.asarray(preds).astype(\"float32\")\n",
        "    return np.random.choice(indices, p=preds)\n",
        "\n",
        "  # predict one step/sentence based on the model type\n",
        "  def predict_one_step(self, model, input, h_states, c_states):\n",
        "\n",
        "    embd_layer = model.layers[0](input)\n",
        "    embd_layer = embd_layer.numpy().reshape(1, embd_layer.numpy().shape[0], embd_layer.numpy().shape[1])\n",
        "    if (h_states is None) & (c_states is None):\n",
        "        h_states, c_states = model.layers[1].get_initial_state(embd_layer)\n",
        "    lstm_out, h_states, c_states = model.layers[1](embd_layer, initial_state=[h_states, c_states])\n",
        "    predicted_logits = model.layers[2](lstm_out)\n",
        "    return predicted_logits, h_states, c_states\n",
        "\n",
        "  # converts to lower case, remove non-asci chars and html tags\n",
        "  def text_standardization(self, text):\n",
        "    \n",
        "    HTML = re.compile('<.*?>')\n",
        "    # initializing punctuations string\n",
        "    PUNCT = '''!()-[]{};:\"\\<>/?@#$%^&*_~'''\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(HTML, ' ', text)\n",
        "    text = ''.join([i if ord(i) < 128 else ' ' for i in text])\n",
        "    for ele in text:\n",
        "      if ele in PUNCT:\n",
        "          text = text.replace(ele, \"\")\n",
        "    return text\n",
        "\n",
        "\n",
        "## https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
        "\n",
        "## Create two seperate embedding layers: one for tokens and one for token index (positions).\n",
        "\n",
        "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "## https://www.tensorflow.org/text/tutorials/text_generation\n",
        "\n",
        "class WordRNN(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, vocab_size, embed_dim, maxlen, dropout, rnn_units, kernel_reg=None):\n",
        "    super().__init__(self)\n",
        "    self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "\n",
        "    # LSTM layer initialisation\n",
        "    self.lstm = tf.keras.layers.LSTM(rnn_units, dropout=dropout,\n",
        "                                    return_sequences=True,\n",
        "                                    return_state=True)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(vocab_size, kernel_regularizer=kernel_reg)\n",
        "\n",
        "  def call(self, inputs, h_states=None, c_states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding_layer(x, training=training)\n",
        "    \n",
        "    if (h_states is None) & (c_states is None):\n",
        "      h_states, c_states = self.lstm.get_initial_state(x)\n",
        "    x, h_states, c_states = self.lstm(x, initial_state=[h_states, c_states], training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "    \n",
        "    if return_state:\n",
        "      return x, h_states, c_states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "bAaES8d0np1-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "BXf3JfW1YnE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BeamSearch"
      ],
      "metadata": {
        "id": "WHd7EF0fYpWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "TextGenerator(sample_input_text='Black Panther', predict_next_words=80, with_beam=True).generate_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "fOPl3B8Kpyex",
        "outputId": "804461ff-c8bd-441b-d0bf-b1f50bee3019"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [10:29<00:00,  7.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run time:, 631.354917049408\n",
            "\n",
            "CPU times: user 10min 30s, sys: 2.57 s, total: 10min 33s\n",
            "Wall time: 10min 31s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"black panther a glance , chip . you mean , i guess it was . targett glanced around the lab at ' and then the sub vanished into view of and i was sure it ' ll have been there for the first five days , so it would take much to . but the other two , and the crawler went on and on , i was on my side and out there in my\""
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GreedySearch"
      ],
      "metadata": {
        "id": "g_OyuJ4MYysi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "TextGenerator(sample_input_text = 'Black Panther', predict_next_words=80, with_beam=False).generate_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "8FvTld0xYwyf",
        "outputId": "1fdece84-ad26-47d9-8d8f-5be604ddb52f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:13<00:00,  6.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run time:, 15.370424270629883\n",
            "\n",
            "CPU times: user 18 s, sys: 746 ms, total: 18.8 s\n",
            "Wall time: 15.4 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"black panther i guess i was . you can ' t make it a ship to take you back out of it . winch . vane grunted . vane nodded , and the other looked . a second later , he said , i don ' t have a gun , mr . vane . it ' s all right , but i ' m not a detective man . the bellhop ' s eyes narrowed suspiciously at the jar .\""
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}